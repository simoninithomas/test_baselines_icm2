import tensorflow as tf
import numpy as np

class ICM(object):
    def __init__(self, ob_space, ac_space):
        input_shape = [None, *ob_space]
        
        # Placeholders
        self.state_ = phi_state = tf.placeholder(tf.float32, input_shape, name="icm_state")
        self.next_state_ = phi_next_state = tf.placeholder(tf.float32, input_shape, name="icm_next_state")
        self.action_ = action = tf.placeholder(tf.float32, [None, ac_space], name="icm_action")
        
        # Feature encoding
        # Aka pass state and next_state to create phi(state), phi(next_state)
        # state to phi(state)
        phi_state = feature_encoding(state_)
        with tf.variable_scope(tf.get_variable_scope(), reuse=True):
            # next_state to phi(next_state)
            phi_next_state = feature_encoding(next_state_)
        
        # INVERSE MODEL
        # Inverse
        pred_action = inverse_model(phi_state, phi_next_state)
        
        # Loss (Cross entropy between predicted action and real action)
        # Transform our one hot encoded actions into index
        # aka [[0 1 0]
        #      [1 0 0]
        #      [0 0 1]]
        # to [1 0 2] 
        actions_index = tf.argmax(action, axis = 1)
        
        self.inv_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, actions_index), name="inverse_loss")
        
        
        # FORWARD MODEL
        # Forward
        pred_phi_next_state = (action, phi_state)
        
        # Loss
        self.forw_loss = tf.reduce_mean(tf.square(tf.subtract(pred_phi_next_state, phi_next_state)), name="forward_loss")
        
        # Backpropagation
        # ????
        self.forw_loss = self.forwardloss * 288.0 # lenFeatures=288. Factored out to make hyperparams not depend on it.

        # Get the variable_list
        self.variable_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)
        print("varlist", self.variable_list)
        
    
    def conv2d(inputs, filters, kernel_size, strides, padding):
        return tf.layers.conv2d(inputs = inputs,
                           filters = filters,
                            kernel_size = (kernel_size, kernel_size),
                            strides = strides,
                            padding = padding)

    # We use batch normalization to do feature normalization as explained in the paper
    def feature_encoding(x):
        x = tf.nn.elu(tf.nn.batch_normalization(conv2d(x, 8, 5, 4, "valid")))
        x = tf.nn.elu(tf.nn.batch_normalization(conv2d(x, 16, 3, 2, "valid")))
        x = tf.nn.elu(tf.nn.batch_normalization(conv2d(x, 32, 3, 2, "valid")))
        x = tf.nn.elu(tf.nn.batch_normalization(conv2d(x, 64, 3, 2, "valid")))
        x = tf.layers.flatten(x)
        x = tf.nn.elu(tf.nn.batch_normalization(tf.layers.dense(x, 256, normalized_columns_initializer(0.01))))
        return x
        
    # Foward Model
    # Given action and phi(st) must find pred_phi(st+1)
    """
    Parameters
    __________
    
    action:   The action taken by our agent
    phi_state: The feature representation of our state generated by our feature_encoding function.
    phi_next_state: The feature representation of our next_state generated by our feature_encoding function.
    
    returns pred_phi_next_state: The feature representation prediction of our next_state.
    """
    def forward_model(action, phi_state):
        
        # Concatenate phi_state and action
        icm_forw_concatenate = tf.concat([phi_state, action])
        
        # FC
        icm_forw_fc1 = tf.layers.dense(icm_forw_concatenate, 256)
        
        # FC (size of phi_state [1] aka the width)
        icm_forw_pred_next_state = tf.layers.dense(icm_forw_fc1, phi_state.get_shape()[1].value)
        
        return icm_forw_pred_next_state
        
    # Inverse Model
    # Given phi(state) and phi(next_state) returns the predicted action Ã¢t
    """
    Parameters
    __________
    
    action:   The real action taken by our agent
    phi_state: The feature representation of our state generated by our feature_encoding function.
    phi_next_state: The feature representation of our next_state generated by our feature_encoding function.
    
    returns pred_action: The predicted action.
    """
    def inverse_model(phi_state, phi_next_state):
        # Concatenate phi(st) and phi(st+1)
        icm_inv_concatenate = tf.concat([phi_state, phi_next_state], 1)
        icm_inv_fc1 = tf.nn.relu(tf.layers.dense(icm_inv_concatenate, 256))
        pred_action_logits = tf.layers.dense(icm_inv_fc1, self.action_space)
        
        pred_action = tf.nn.softmax(logits, dim=-1)
        
        return pred_action
    
    
    # Calculate intrinsic reward
    """
    Parameters
    __________
    
    phi_next_state: The feature representation of our next_state generated by our feature_encoding function.
    pred_phi_next_state:   The feature representation prediction of our next_state.
    
    
    returns intrinsic_reward: The intrinsic reward
    """
    def calculate_intrinsic_reward(self, state, next_state, action):
        sess = tf.get_default_session()

        error = sess.run(self.forw_loss, {self.state_: state, self.next_state_: next_state_, self.action_: action})
        error = error * constants['PREDICTION_BETA']
        return error

        
        
        